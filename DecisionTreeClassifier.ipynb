{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3a5441c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d5ad2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#***************************************************************************************************\n",
    "# Definition of Tree class\n",
    "# This is used to hold the decision tree\n",
    "#------------\n",
    "#Parameters:\n",
    "#------------\n",
    "# left:  Reference to the left of the tree\n",
    "# right: Reference to the left of the tree\n",
    "# data : Can hold all the data we want to store in each node or leaf\n",
    "# seq_num: Used to mark all nodes sequentially in preorder traversal\n",
    "#        : This will be used to make right connections while drawing the graph using graphviz\n",
    "# parent : Parent node for any node/leaf, directly connected at the level above\n",
    "#***************************************************************************************************\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.data = None\n",
    "        self.seq_num = 0\n",
    "        self.parent = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660a5a0",
   "metadata": {},
   "source": [
    "##### criterion{“gini”, “entropy”, “log_loss”,\"gain_ratio}, default=”gini”\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity, “log_loss” and “entropy” both for the Shannon information gain, gain_ratio for normalize information gain  see Mathematical formulation.\n",
    "\n",
    "##### splitter{“best”, “random”}, default=”best”\n",
    "The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
    "\n",
    "##### max_depth int, default=None\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "##### min_samples_split int, default=2\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "##### min_samples_leaf int , default=1\n",
    "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c5c241fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "class DecisionTreeClassifier_my:\n",
    "    criterion = 0  # 0 = gini, 1 = entropy, 2 = log_loss, 3 = gain_raio\n",
    "    splitter  = 0  # 0 = best, 1 = random\n",
    "    max_depth = 0  # 0 = None\n",
    "    min_sample_split = 2  \n",
    "    min_samples_leaf = 1\n",
    "    label = \"UNS\"\n",
    "    feature_label = \"feature\"\n",
    "    debug = 1\n",
    "    __tree_root = None\n",
    "    x_train = None\n",
    "    y_train = None\n",
    "        \n",
    "    def __init__(self,criterion=0,splitter=0,max_depth=0,min_sample_split=2,min_samples_leaf=1):\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        \n",
    "\n",
    "    def get_feature_names(self,X_data):\n",
    "        x_num_rows, x_num_cols = X_data.shape\n",
    "        col_names = []\n",
    "        for col_num in range(1,x_num_cols+1):\n",
    "            col_names.append(self.feature_label+str(col_num))\n",
    "        return col_names\n",
    "    \n",
    "    def create_dataframe_format(self,X_train, y_train):\n",
    "        col_names = self.get_feature_names(X_train)\n",
    "        my_X_train = pd.DataFrame(X_train, columns =[col_names])\n",
    "        my_y_train = pd.DataFrame(y_train, columns=[self.label])\n",
    "        my_y_train = my_y_train.reset_index() \n",
    "        train_df = pd.concat([my_X_train, my_y_train],axis=1, join='inner')\n",
    "        train_df.drop('index', 1,inplace=True)\n",
    "        return train_df\n",
    "    \n",
    "    #***************************************************************************************************\n",
    "    # return_value : log_loss calculated value\n",
    "    #--------------\n",
    "    # Parameters:\n",
    "    #--------------\n",
    "    # data: dataframe \n",
    "    # feature: for which the log_loss is to be calculated\n",
    "    #***************************************************************************************************\n",
    "\n",
    "    def get_log_loss(self,data,feature):\n",
    "        # --------------------------------------------------------\n",
    "        # H(x) = - 1/N (Sum (xi*log(p(xi)) + (1-xi)*log(1- p(xi))\n",
    "        # ---------------------------------------------------------\n",
    "        total = data[feature].count()\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        categories = data[feature].cat.categories.tolist()\n",
    "        counts = data[feature].value_counts()\n",
    "        log_loss = 0\n",
    "\n",
    "        for cat in categories:\n",
    "            px = counts[cat]/total\n",
    "            qx = 1-px\n",
    "\n",
    "            if px == 0:\n",
    "                log_loss_1 = 0\n",
    "            else:\n",
    "                log_loss_1 = -(px*math.log(px,2))\n",
    "\n",
    "            if qx == 0:\n",
    "                log_loss_2 = 0\n",
    "            else:\n",
    "                log_loss_2 = -(qx)*math.log((qx),2)\n",
    "\n",
    "            log_loss += log_loss_1 + log_loss_2\n",
    "\n",
    "        return log_loss\n",
    "\n",
    "    #***************************************************************************************************\n",
    "    # return_value : Entroy calculated value\n",
    "    #--------------\n",
    "    # Parameters:\n",
    "    #--------------\n",
    "    # data: dataframe \n",
    "    # feature: for which the entropy is to be calculated\n",
    "    #***************************************************************************************************\n",
    "\n",
    "\n",
    "    def get_entropy(self,data,feature):\n",
    "        # -----------------------------------\n",
    "        # H(x) = - Sum p(xi)*log(p(xi))\n",
    "        # -----------------------------------\n",
    "        total = data[feature].count()\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        categories = data[feature].cat.categories.tolist()\n",
    "        counts = data[feature].value_counts()\n",
    "        entropy = 0\n",
    "\n",
    "        for cat in categories:\n",
    "            px = counts[cat]/total\n",
    "            if px == 0:\n",
    "                entropy += 0\n",
    "            else:\n",
    "                entropy += -(px*math.log(px,2))\n",
    "\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    #***************************************************************************************************\n",
    "    # return_value : (1) value, which gives max gain ratio   (2) gain_ratio at that value\n",
    "    #              : \n",
    "    #--------------\n",
    "    # Parameters:\n",
    "    #--------------\n",
    "    # df: dataframe \n",
    "    # feature: for which the gain_ratio is to be calculated\n",
    "    # y      : label column\n",
    "\n",
    "    # For now, It is implemented only for continuous features, as our problem has only that data type\n",
    "    #***************************************************************************************************\n",
    "\n",
    "    def get_gain_ratio(self,df,feature,y):\n",
    "        values = df[feature].unique()\n",
    "        global_entropy = self.get_entropy(y,self.label)\n",
    "\n",
    "        min_val = min(values)\n",
    "        max_val = max(values)\n",
    "        total = df[feature].count()\n",
    "\n",
    "        max_gain_ratio = 0\n",
    "        return_value = 0\n",
    "        #print(values)\n",
    "\n",
    "        new_df = pd.concat([df, y], axis=1,join='inner')\n",
    "\n",
    "        #one way is to take between each of two values, as prof suggested in class. Let's try that out\n",
    "        mean_values = []\n",
    "        value_index = 0\n",
    "        while value_index < len(values)-1:\n",
    "            mean_value = (values[value_index] + values [value_index+1])/2\n",
    "            mean_values.append(mean_value)\n",
    "            value_index += 1\n",
    "\n",
    "        for value in mean_values:\n",
    "            new_df_low = new_df[new_df[feature] <=value]\n",
    "            low_entropy = self.get_entropy(new_df_low,self.label)\n",
    "            low_count = new_df_low[feature].count()\n",
    "\n",
    "            new_df_high = new_df[new_df[feature] >value]\n",
    "            high_entropy = self.get_entropy(new_df_high,self.label)\n",
    "            high_count = new_df_high[feature].count()\n",
    "\n",
    "            gain = global_entropy - (low_count/total)*low_entropy -(high_count/total)*high_entropy\n",
    "            if low_count == 0:\n",
    "                split_info = - (high_count/total)*math.log((high_count/total),2)\n",
    "            elif high_count == 0:\n",
    "                split_info = - (low_count/total)*math.log((low_count/total),2)\n",
    "            else:\n",
    "                split_info = -(low_count/total)*math.log((low_count/total),2) - (high_count/total)*math.log((high_count/total),2)\n",
    "\n",
    "            gain_ratio = 0\n",
    "            if split_info  !=0:\n",
    "                gain_ratio = gain/split_info \n",
    "            #print(feature, value, low_count, high_count, gain, gain_ratio)\n",
    "            if gain_ratio > max_gain_ratio:\n",
    "                max_gain_ratio = gain_ratio\n",
    "                return_value = value\n",
    "\n",
    "        return return_value, max_gain_ratio\n",
    "\n",
    "    #***************************************************************************************************\n",
    "    # return_value : definition for next node in the decision tree, following values are returned:\n",
    "    #                (1) feature_selected  (2) feature_value (3) gain_ratio\n",
    "    #--------------\n",
    "    # Parameters:\n",
    "    #--------------\n",
    "    # X: features dataframe\n",
    "    # y      : label column\n",
    "    #***************************************************************************************************\n",
    "    def get_node_definition(self,X,y):\n",
    "        max_gain_ratio = 0\n",
    "        feature_value = 0\n",
    "        feature_selected = None\n",
    "\n",
    "        for feature in X:\n",
    "            value, gain_ratio = self.get_gain_ratio(X,feature,y)\n",
    "            if gain_ratio >max_gain_ratio:\n",
    "                max_gain_ratio = gain_ratio\n",
    "                feature_value = value\n",
    "                feature_selected = feature\n",
    "\n",
    "        return feature_selected,feature_value, max_gain_ratio\n",
    "\n",
    "    #***************************************************************************************************\n",
    "    # return_value : Tree object, which has the final result of decision tree algorithm:\n",
    "    #--------------\n",
    "    # Parameters:\n",
    "    #--------------\n",
    "    # df         : dataframe\n",
    "    # level      : parameter used internally to stop at a particular level\n",
    "    #***************************************************************************************************\n",
    "\n",
    "    def __fit__(self,df,level=0):\n",
    "            X = df.drop(self.label, axis=1)\n",
    "            y = df[[self.label]]\n",
    "            y.loc[:,(self.label)] = y[self.label].astype('category')\n",
    "\n",
    "            feature,value, gain_ratio = self.get_node_definition(X,y)\n",
    "            node_entropy = self.get_entropy(y,self.label)\n",
    "\n",
    "            ################### Data storage part ###############################\n",
    "            vc = y[self.label].value_counts()\n",
    "            vc_d = dict(vc)\n",
    "            values = [0,0,0,0] #Very low, low, middle, high\n",
    "            key = \"Very Low\"\n",
    "            if key in vc_d:\n",
    "                values[3] = vc_d[key]\n",
    "\n",
    "            key = \"Low\"\n",
    "            if key in vc_d:\n",
    "                values[1] = vc_d[key]\n",
    "\n",
    "            key = \"Middle\"\n",
    "            if key in vc_d:\n",
    "                values[2] = vc_d[key]\n",
    "\n",
    "            key = \"High\"\n",
    "            if key in vc_d:\n",
    "                values[0] = vc_d[key]\n",
    "\n",
    "            my_class = max(vc_d, key=vc_d.get)\n",
    "            ################### END: Data storage part ###############################\n",
    "            if df[self.label].nunique() == 1:\n",
    "                node = Tree()\n",
    "                node.data = (\"LEAF\",0,0,0,[values],my_class)\n",
    "                return node\n",
    "\n",
    "            node = Tree()\n",
    "            node.data = (feature,value,gain_ratio,node_entropy,values,my_class)\n",
    "\n",
    "            left_df = df[df[feature] <= value]   \n",
    "            right_df =df[df[feature] > value]\n",
    "\n",
    "            if self.max_depth > 0:\n",
    "                if level == self.max_depth:\n",
    "                    return node\n",
    "                level = level+1\n",
    "\n",
    "            if left_df.empty:\n",
    "                node.left = None\n",
    "            else:\n",
    "                node.left =  self.__fit__(left_df,level)\n",
    "                node.left.parent = node\n",
    "\n",
    "\n",
    "            if right_df.empty:\n",
    "                node.right = None\n",
    "            else:\n",
    "                node.right =  self.__fit__(right_df,level)\n",
    "                node.right.parent = node\n",
    "\n",
    "            return node\n",
    "\n",
    "\n",
    "    def fit(self,X_train,y_train):\n",
    "            #x_num_rows, x_num_cols = X_train.shape\n",
    "            #y_num_row, y_num_cols = y_train.shape\n",
    "\n",
    "            #validation\n",
    "            #if x_num_rows != y_num_rows:\n",
    "            #    print(\"Shape mismatch between training and test data, please correct!\")\n",
    "            #    return -1\n",
    "            #if y_num_cols != 1:\n",
    "            #    print(\"Shape mismatch between training and test data, please correct!\")\n",
    "            #    return -1\n",
    "\n",
    "            #check y data type, must be object or category\n",
    "            #TBD\n",
    "            \n",
    "            self.X_train = X_train\n",
    "            self.y_train = y_train\n",
    "            \n",
    "            #Prepare custom dataframe\n",
    "            train_df = self.create_dataframe_format(X_train,y_train)\n",
    "            \n",
    "            self.__tree_root = self.__fit__(train_df)\n",
    "            #call internal fit method for execution\n",
    "\n",
    "\n",
    "\n",
    "    #***************************************************************************************************\n",
    "    # return_value : None, it just prints the tree with all nodes in (level, data) format\n",
    "    #--------------\n",
    "    # Parameters:\n",
    "    #--------------\n",
    "    # root       : Tree\n",
    "    # level      : Used for recursively calling itself with the level at which next node is\n",
    "    #***************************************************************************************************\n",
    "    def print_tree(root,level):\n",
    "        if root is None:\n",
    "            return\n",
    "        print(level,root.data)\n",
    "        level = level+1\n",
    "        print_tree(root.left,level)\n",
    "        print_tree(root.right,level)\n",
    "\n",
    "    #***************************************************************************************************\n",
    "    # return_value : Predicted class value for given set of feature\n",
    "    #              : It uses the given decision tree with feature set provided to predict a output label\n",
    "    #--------------\n",
    "    # Parameters:\n",
    "    #--------------\n",
    "    # row        : all features set for one row\n",
    "    # root       : decision tree\n",
    "    #***************************************************************************************************    \n",
    "    def predict_tuple(self,row,root,level=0,depth=100):\n",
    "        param,val, gr, ent,res, my_class = root.data\n",
    "\n",
    "        if param == \"LEAF\":\n",
    "            return my_class\n",
    "\n",
    "        if  level == depth:\n",
    "            return my_class\n",
    "\n",
    "        if root.left == None or root.right == None:\n",
    "            return my_class\n",
    "\n",
    "        level = level+1\n",
    "        if row[param] <= val:\n",
    "            return self.predict_tuple(row,root.left,level,depth)\n",
    "        else:\n",
    "            return self.predict_tuple(row,root.right,level,depth)\n",
    "\n",
    "\n",
    "    #***************************************************************************************************\n",
    "    # return_value : (1) Predicted class value for whole of given dataframe (2) Confusion matrix\n",
    "    #              : It uses the decision tree with feature set provided to predict all output labels\n",
    "    #--------------\n",
    "    # Parameters:\n",
    "    #--------------\n",
    "    # row        : all features set for whole training set\n",
    "    # root       : decision tree\n",
    "    #*************************************************************************************************** \n",
    "    def predict(self, X_test, depth=0):\n",
    "        col_names = self.get_feature_names(X_train)\n",
    "        df = pd.DataFrame(X_test, columns =[col_names])\n",
    "        \n",
    "        guess_list = []\n",
    "        for index, row in df.iterrows():\n",
    "            guess_class = self.predict_tuple(row,self.__tree_root,depth)\n",
    "            guess_list.append(guess_class)\n",
    "             \n",
    "        return guess_list\n",
    "        \n",
    "    def accuracy(self,guess_list,actual_list):\n",
    "\n",
    "        class_labels = actual_list.unique()\n",
    "        rows = len(class_labels)\n",
    "        cols = rows\n",
    "        \n",
    "        actual_list = actual_list.tolist()\n",
    "        total_rows = len(actual_list)\n",
    "        correct = 0\n",
    "\n",
    "        confusion_matrix_data = pd.DataFrame([[0]*cols]*rows,\n",
    "                      index=pd.Index(class_labels, name='Actual Values'),\n",
    "                      columns=pd.Index(class_labels, name='Predicted Values'))\n",
    "\n",
    "        i = 0\n",
    "        for guess_class in guess_list:\n",
    "            actual_class = actual_list[i]\n",
    "            \n",
    "            if guess_class == actual_class:\n",
    "                correct = correct+1\n",
    "\n",
    "            i = i+1\n",
    "            value = confusion_matrix_data[actual_class][guess_class] + 1\n",
    "            confusion_matrix_data[actual_class][guess_class] = value\n",
    "\n",
    "        confusion_matrix_data.sort_index(axis=0, inplace=True)\n",
    "        confusion_matrix_data.sort_index(axis=1, inplace=True)\n",
    "\n",
    "        accuracy = correct/total_rows\n",
    "        return accuracy, confusion_matrix_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "323ce31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\ashukuma\\\\Documents\\\\Personal\\\\BITS-Pilani\\\\Classification\\\\Assignment3\\\\Predict_student_ knowledge_level.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d2de7914",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns=lambda x: x.strip())\n",
    "#drop unwanted cells\n",
    "df.drop('Unnamed: 6', axis=1,inplace=True)\n",
    "df.drop('Unnamed: 7', axis=1,inplace=True)\n",
    "df.drop('Unnamed: 8', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "92cb54b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNS     \n",
       "Low         129\n",
       "Middle      122\n",
       "High        102\n",
       "Very Low     50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seems some typing mistakes for Very low category, let's clean it\n",
    "df['UNS']=df['UNS'].replace(['very_low'], 'Very Low')\n",
    "df.value_counts(['UNS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d4640e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "4b76b978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop('UNS', axis=1)\n",
    "y = df['UNS']\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=.25, random_state=0)\n",
    "sss.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8e603637",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Low', 'Middle', 'Low', 'Middle', 'Middle', 'High', 'Low', 'Low', 'Middle', 'Low', 'Very Low', 'High', 'Low', 'High', 'Middle', 'Low', 'Middle', 'Low', 'Middle', 'High', 'Middle', 'Low', 'Very Low', 'High', 'Low', 'Low', 'Low', 'Low', 'Low', 'Low', 'High', 'Middle', 'Middle', 'Very Low', 'Low', 'Low', 'High', 'Very Low', 'Very Low', 'Low', 'Low', 'Low', 'Middle', 'Middle', 'Middle', 'Middle', 'Middle', 'High', 'High', 'Low', 'Middle', 'High', 'Low', 'Middle', 'Low', 'High', 'Middle', 'High', 'Middle', 'Middle', 'Middle', 'Middle', 'Very Low', 'High', 'Low', 'High', 'Middle', 'High', 'Low', 'High', 'High', 'Low', 'Middle', 'Low', 'Very Low', 'Low', 'Low', 'High', 'Middle', 'Low', 'High', 'Middle', 'High', 'High', 'Low', 'Low', 'Middle', 'Low', 'Middle', 'Very Low', 'Very Low', 'Very Low', 'Low', 'High', 'Middle', 'High', 'Middle', 'Low', 'Very Low', 'High', 'Middle']\n"
     ]
    }
   ],
   "source": [
    "clf_model = DecisionTreeClassifier_my(criterion=\"gini\",  min_samples_leaf=5)   \n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    clf_model.fit(X_train,y_train)\n",
    "    pred = clf_model.predict(X_test)\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "639764c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9504950495049505\n",
      "Predicted Values  High  Low  Middle  Very Low\n",
      "Actual Values                                \n",
      "High                24    0       0         0\n",
      "Low                  0   32       2         1\n",
      "Middle               2    0      29         0\n",
      "Very Low             0    0       0        11\n"
     ]
    }
   ],
   "source": [
    "accuracy, confusion = clf_model.accuracy(pred,y_test)\n",
    "print(accuracy)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2c8472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
