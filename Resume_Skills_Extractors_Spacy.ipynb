{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashuonnet/ailearn/blob/main/Resume_Skills_Extractors_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpbDtHo_7DAi",
        "outputId": "13020cbe-fbe4-47c3-b4ef-2f45dcbc5f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy_transformers\n",
            "  Downloading spacy_transformers-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from spacy_transformers) (2.4.5)\n",
            "Collecting spacy<4.0.0,>=3.5.0\n",
            "  Downloading spacy-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from spacy_transformers) (1.13.1+cu116)\n",
            "Collecting transformers<4.27.0,>=3.4.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy_transformers) (1.22.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.7.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (4.64.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.10.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.25.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.8.0->spacy_transformers) (4.5.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<4.27.0,>=3.4.0->spacy_transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<4.27.0,>=3.4.0->spacy_transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers<4.27.0,>=3.4.0->spacy_transformers) (6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.1)\n",
            "Installing collected packages: tokenizers, spacy-alignments, huggingface-hub, transformers, spacy, spacy_transformers\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.4\n",
            "    Uninstalling spacy-3.4.4:\n",
            "      Successfully uninstalled spacy-3.4.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.12.1 spacy-3.5.0 spacy-alignments-0.9.0 spacy_transformers-1.2.2 tokenizers-0.13.2 transformers-4.26.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.5.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy_transformers\n",
        "!pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJCqVcRk7dhk"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVhpk1MNMn3S",
        "outputId": "21d54aa5-8a62-4eae-b9d9-e89d13045272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Feb 28 14:24:39 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    29W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2X_ON3vMwlh",
        "outputId": "959cfa66-d341-44da-efb8-c2b464cfdc8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'skill_extractor': No such file or directory\n",
            "Cloning into 'skill_extractor'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 28 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), 503.33 KiB | 1.77 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!rm -r skill_extractor\n",
        "!git clone https://github.com/ashuonnet/skill_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1nnMdEjrZsB",
        "outputId": "804ebf2b-7521-4728-b656-83823f1ed1b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-02-28 14:24:46.277997: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-28 14:24:47.152402: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-28 14:24:47.152529: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-28 14:24:47.152550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/skill_extractor/data/train/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy init fill-config /content/skill_extractor/data/train/base_config.cfg /content/skill_extractor/data/train/config.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ts-pzpgjx0U"
      },
      "source": [
        "## Helper Functions for converting annotations to spacy format. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoPwWLwgjvjQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# JSON formatting functions\n",
        "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
        "    training_data = []\n",
        "    lines=[]\n",
        "    with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        data = json.loads(line)\n",
        "        text = data['content'].replace(\"\\n\", \" \")\n",
        "        entities = []\n",
        "        data_annotations = data['annotation']\n",
        "        if data_annotations is not None:\n",
        "            for annotation in data_annotations:\n",
        "                #only a single point in text annotation.\n",
        "                point = annotation['points'][0]\n",
        "                labels = annotation['label']\n",
        "                # handle both list of labels or a single label.\n",
        "                if not isinstance(labels, list):\n",
        "                    labels = [labels]\n",
        "\n",
        "                for label in labels:\n",
        "                    point_start = point['start']\n",
        "                    point_end = point['end']\n",
        "                    point_text = point['text']\n",
        "\n",
        "                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                    if lstrip_diff != 0:\n",
        "                        point_start = point_start + lstrip_diff\n",
        "                    if rstrip_diff != 0:\n",
        "                        point_end = point_end - rstrip_diff\n",
        "                    entities.append((point_start, point_end + 1 , label))\n",
        "        training_data.append((text, {\"entities\" : entities}))\n",
        "    return training_data\n",
        "\n",
        "def trim_entity_spans(data: list) -> list:\n",
        "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
        "\n",
        "    Args:\n",
        "        data (list): The data to be cleaned in spaCy JSON format.\n",
        "\n",
        "    Returns:\n",
        "        list: The cleaned data.\n",
        "    \"\"\"\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                    text[valid_start]):\n",
        "                valid_start += 1\n",
        "            while valid_end > 1 and invalid_span_tokens.match(\n",
        "                    text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "    return cleaned_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3McC-2W8pY4"
      },
      "outputs": [],
      "source": [
        "#cv_data = json.load(open('/content/skill_extractor/data/train/resumes_1.json'))\n",
        "cv_data = trim_entity_spans(convert_dataturks_to_spacy('/content/skill_extractor/data/train/resumes_1.json'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1T9DUh0qpIv",
        "outputId": "992693c4-957f-4c23-a01e-bdb32a30e69d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Afreen Jamadar Active member of IIIT Committee in Third year  Sangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6  I wish to use my knowledge, skills and conceptual understanding to create excellent team environments and work consistently achieving organization objectives believes in taking initiative and work to excellence in my work.  WORK EXPERIENCE  Active member of IIIT Committee in Third year  Cisco Networking -  Kanpur, Uttar Pradesh  organized by Techkriti IIT Kanpur and Azure Skynet. PERSONALLITY TRAITS: • Quick learning ability • hard working  EDUCATION  PG-DAC  CDAC ACTS  2017  Bachelor of Engg in Information Technology  Shivaji University Kolhapur -  Kolhapur, Maharashtra  2016  SKILLS  Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT ACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)  ADDITIONAL INFORMATION  TECHNICAL SKILLS:  • Programming Languages: C, C++, Java, .net, php. • Web Designing: HTML, XML • Operating Systems: Windows […] Windows Server 2003, Linux. • Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.  https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN',\n",
              " {'entities': [[1155, 1199, 'Email Address'],\n",
              "   [743, 1141, 'Skills'],\n",
              "   [729, 733, 'Graduation Year'],\n",
              "   [675, 702, 'College Name'],\n",
              "   [631, 673, 'Degree'],\n",
              "   [625, 629, 'Graduation Year'],\n",
              "   [614, 623, 'College Name'],\n",
              "   [606, 612, 'Degree'],\n",
              "   [104, 148, 'Email Address'],\n",
              "   [62, 68, 'Location'],\n",
              "   [0, 14, 'Name']]}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cv_data[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvr_liDw8YXn"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RePWCvix8ooq",
        "outputId": "8f26c2a2-4cda-473b-f8ae-9509b5381436"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.5.0'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spacy.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFwnkY5P7E5V"
      },
      "outputs": [],
      "source": [
        "def get_spacy_doc(file, data):\n",
        "    nlp = spacy.blank('en')\n",
        "    db = DocBin()\n",
        "        \n",
        "    for text, annot in tqdm(data):\n",
        "        doc = nlp.make_doc(text)\n",
        "        annot = annot['entities']\n",
        "    \n",
        "        ents = []\n",
        "        entity_indices = []\n",
        "            \n",
        "        for start, end, label in annot:\n",
        "            skip_entity = False\n",
        "            for idx in range(start, end):\n",
        "                if idx in entity_indices:\n",
        "                    skip_entity = True\n",
        "                    break\n",
        "            if skip_entity == True:\n",
        "                continue \n",
        "                    \n",
        "            entity_indices = entity_indices + list(range(start, end))  \n",
        "                \n",
        "            try:\n",
        "                span = doc.char_span(start, end,label=label,alignment_mode = 'strict')\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            if span is None:\n",
        "                err_data = str([start, end]) + \" \" + str(text) + \"\\n \"\n",
        "                err_data = str(err_data.encode(\"UTF-8\"))\n",
        "                file.write(err_data)\n",
        "            else:\n",
        "                ents.append(span)\n",
        "\n",
        "        try:\n",
        "            doc.ents = ents\n",
        "            db.add(doc)\n",
        "        except:\n",
        "            pass \n",
        "    return db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hhjbjwd7Yj-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(cv_data, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYofwTnG7Zp5",
        "outputId": "e02425f3-2316-4bcc-b414-8209e045f99a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(154, 66)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train),len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLuVjfO-7c_f",
        "outputId": "8e44d9b4-6fd3-43ae-cb9b-cb46e3c1d72a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 154/154 [00:02<00:00, 75.06it/s]\n",
            "100%|██████████| 66/66 [00:00<00:00, 70.83it/s]\n"
          ]
        }
      ],
      "source": [
        "file = open('error.txt','w')\n",
        "\n",
        "db = get_spacy_doc(file, train)\n",
        "db.to_disk('/content/skill_extractor/data/train/train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file, test)\n",
        "db.to_disk('/content/skill_extractor/data/train/test_data.spacy')\n",
        "\n",
        "file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hc4P4X87lLa",
        "outputId": "8d8d86f3-035d-4a66-d396-89f7528b20de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(db.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7zUus2D1kBt"
      },
      "outputs": [],
      "source": [
        "#Monkey patch for below issue, but works fine. Error reported for shell cmd\n",
        "############################################################################\n",
        "# NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\n",
        "############################################################################\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVK6v6ht7qlH",
        "outputId": "7fb0ccc3-bf14-4d73-dbc1-bfddc4580bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-02-28 14:25:13.093654: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-28 14:25:13.093758: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-28 14:25:13.093778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-02-28 14:25:22,158] [INFO] Set up nlp object from config\n",
            "[2023-02-28 14:25:22,174] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2023-02-28 14:25:22,179] [INFO] Created vocabulary\n",
            "[2023-02-28 14:25:22,181] [INFO] Finished initializing nlp object\n",
            "Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 70.6kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 6.15MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 3.23MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 8.47MB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 501M/501M [00:06<00:00, 77.0MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-02-28 14:26:00,792] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        9238.60   1544.73    0.37    0.19    6.47    0.00\n",
            "  3     200      246073.13  65751.87   25.14   28.40   22.55    0.25\n",
            "  6     400       63978.17  25045.74   54.57   52.38   56.96    0.55\n",
            " 10     600        4691.84  20084.02   56.71   61.46   52.64    0.57\n",
            " 13     800        8896.18  17247.90   58.26   54.51   62.57    0.58\n",
            " 16    1000        2549.03  16956.80   59.66   59.34   59.98    0.60\n",
            " 20    1200        2076.11  14668.10   54.99   67.50   46.39    0.55\n",
            " 23    1400        4648.53  14356.67   58.03   54.27   62.35    0.58\n",
            " 27    1600        1769.81  14326.10   54.79   68.56   45.63    0.55\n",
            " 30    1800        1475.40  12802.26   56.27   63.71   50.38    0.56\n",
            " 33    2000         707.40  12826.32   60.51   59.87   61.17    0.61\n",
            " 37    2200         478.94  11975.60   57.92   65.53   51.89    0.58\n",
            " 40    2400        7530.00  11428.34   57.14   63.10   52.21    0.57\n",
            " 44    2600       39552.58  11772.72   58.50   58.53   58.47    0.58\n",
            " 47    2800        2678.87   9933.36   57.09   65.50   50.59    0.57\n",
            " 50    3000        2042.75   9692.33   58.22   62.10   54.80    0.58\n",
            " 54    3200         452.90   8784.33   57.74   58.54   56.96    0.58\n",
            " 57    3400         579.29   7487.16   57.43   64.64   51.67    0.57\n",
            " 61    3600       12711.16   7186.78   58.15   56.06   60.41    0.58\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy train /content/skill_extractor/data/train/config.cfg --output ./output --paths.train /content/skill_extractor/data/train/train_data.spacy --paths.dev /content/skill_extractor/data/train/test_data.spacy --gpu-id 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kGEdJLj8BkQ"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/model-best.zip /content/output/model-best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vANSrX367un9"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('./output/model-best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFt4JJh077Dy"
      },
      "outputs": [],
      "source": [
        "txt = 'My name is Kashmira sharma. I work as a dentist. I completed my degree in Bachelors of Dental Surgery from Government dental College Mumbai. SKILLs : Dental Works'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Vi417jI776f"
      },
      "outputs": [],
      "source": [
        "doc = nlp(txt)\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_.upper(),\"  =>  \", ent.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY6RRSMjObF9"
      },
      "outputs": [],
      "source": [
        "test_resume = 'Ram Krishna Chauhan  '\\\n",
        "'Email ID: testemail@gmail.com '\\\n",
        "'Phone : 09963987441 '\\\n",
        "'Job Objective '\\\n",
        "'To work in the most challenging position with an organization that provides ample opportunities to learn, utilize my skills and abilities with extreme '\\\n",
        "'professionalism and to be highly resourceful, innovative and flexible whenever required. '\\\n",
        "'Experience Summary '\\\n",
        "'6 years of experience in C, Linux, Networking. '\\\n",
        "'Have experience in Packet Forwarding support on Cisco 12000 series router. '\\\n",
        "'Hands on experience in working with Broadcom chipset ARAD+. '\\\n",
        "'Expertise in Handling Customer found defects and fixing them at high priority. '\\\n",
        "'Good knowledge in protocols like IPV4, L3VPN, NAT, L2VPN. '\\\n",
        "'Good understanding of TCAM, Cisco IOS-XR architecture. '\\\n",
        "'Basic Hands on experience in System programming. '\\\n",
        "'Technical skills: '\\\n",
        "'Language '\\\n",
        "'C, Micro Code. '\\\n",
        "'Networking Technologies '\\\n",
        "'IPv4, L3VPN, L2VPN, MPLS, NAT '\\\n",
        "'OS '\\\n",
        "'IOS-XR, Linux '\\\n",
        "'Academic Chronicle '\\\n",
        "'B.Tech. in Electronics & Communication Engineering with 79% '\\\n",
        "'V.R Siddhartha Engineering College - Nagarjuna University. '\\\n",
        "'Intermediate during 2005-2007 with 96% '\\\n",
        "'Sri Engineering Educational Institute - State Board. '\\\n",
        "'S.S.C during 2004-2005 with 86% '\\\n",
        "'St Johnes public school - State Board. '\\\n",
        "'Work Experience: '\\\n",
        "'Gigamon : Feb 2017 - till date '\\\n",
        "'Designation : Senior Engineer. '\\\n",
        "'CODC, HCL Technologies : May 2011 - Feb 2017 '\\\n",
        "'Designation : Lead Engineer. '\\\n",
        "'Project - H Series Devices Development/Sustenance (Gigamon) '\\\n",
        "'Hardware : Gigamon H Series Devices '\\\n",
        "'Software : C language, Linux '\\\n",
        "'Tools : GIT, JIRA '\\\n",
        "'Duration : Feb 2017 to till date '\\\n",
        "'Description : H Series delivers performance and intelligence in each of its Visibility Fabric nodes, with port density and speeds that scales from 1 Gb to '\\\n",
        "'100 Gb. The Visibility Fabric is able to replicate, filter and selectively forward network traffic to monitoring, management and security tools. '\\\n",
        "'Role '\\\n",
        "'Handling both externally found and internally found defects. '\\\n",
        "'Project - ARWEN LC DEVELOPMENT (HCL - CODC) '\\\n",
        "'Hardware : ARWEN LC '\\\n",
        "'Software : C language, Cisco IOS-XR '\\\n",
        "'Tools : ACME, DDTS '\\\n",
        "'Duration : 1 year 3 months '\\\n",
        "'Description : Arwen is internal codename for Scapa packet card based on Broadcom’s Dune family of ARAD+ packet processor to provide combined packet and OTN  '\\\n",
        "'switching functionality. '\\\n",
        "'Role '\\\n",
        "'XR Bring-up of GRIMA FPGA for external stats support. '\\\n",
        "'Integrated multiple feature stats for Arwen LC. '\\\n",
        "'Project - CRS -CGN Development/Sustenance (HCL - CODC) '\\\n",
        "'Hardware : CRS - CGSE + LC. '\\\n",
        "'Software : C language, Cisco IOS-XR '\\\n",
        "'Tools : ACME, DDTS '\\\n",
        "'Duration : 1 year '\\\n",
        "'Description : Carrier Grade Network Address Translation (CGN) is a large scale NAT that is capable of providing private IPv4 to public IPv4 address  '\\\n",
        "'translation in the order of millions of translations to support a large number of subscribers, and at least 10 Gbps full-duplex bandwidth throughput. '\\\n",
        "'Role '\\\n",
        "'Worked on Port-preservation feature and bug fixing. '\\\n",
        "'3. Project - GSR FORWARDING Development/Sustenance (HCL - CODC) '\\\n",
        "'Hardware : C12000 '\\\n",
        "'Software : C language, Cisco IOS, Cisco IOS-XR '\\\n",
        "'Tools : Clear Case, ACME, DDTS '\\\n",
        "'Duration : 2 year 9 months '\\\n",
        "'Description : Gigabit Switch Router (GSR) is a router designed and manufactured by Cisco Systems to provide IP and MPLS services. GSR has a distributed '\\\n",
        "'architecture. Control plane lies on RP (Route processor) and data plane lies on LC\\'s (Line card). GSR uses distributed CEF for performing packet '\\\n",
        "'forwarding. GSR has different flavors of Line-cards E0 to E6 and uses IOS - XR (Inter Networking operating system) as the Operating System. '\\\n",
        "'CLI implementation to reload the LC, once pre-cam failure count reaches threshold value. '\n",
        "'Role '\\\n",
        "'Support for TCAM, Extended Features, L3 packet forwarding on E3 and E5 LC’s. '\\\n",
        "'Handling both externally and internally found defects on E3 and E5. '\\\n",
        "'Experience in Microcode debugging and bug fixing. '\\\n",
        "'Personal details '\\\n",
        "'Father’s Name '\\\n",
        "'P. S. Murthy '\\\n",
        "'Date of Birth ' \n",
        "'25-08-1992 '\\\n",
        "'Marital Status '\\\n",
        "'Single '\\\n",
        "'Languages known '\\\n",
        "'English, Telugu, Hindi, Tamil '\\\n",
        "'E-Mail '\\\n",
        "'testemail@gmail.com '\\\n",
        "'Contact Address '\\\n",
        "'23, 23-4, '\\\n",
        "'Nagarjuna teample Road, '\\\n",
        "'Kakinada, '\\\n",
        "'Vijayawada - 520012 '\\\n",
        "'Declaration: '\\\n",
        "'I hereby declare that the above written particulars are true to the best of my knowledge and belief. '\\\n",
        "'Place: Chennai Regards '\\\n",
        "'Date: Ram Krishna Chauhan '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQVWsIyYTZwD"
      },
      "outputs": [],
      "source": [
        "doc = nlp(test_resume)\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_.upper(),\"  =>  \", ent.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnjF2gXM8AEN"
      },
      "outputs": [],
      "source": [
        "#NLP = spacy.load('./output/model-last')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEE-s_OH8DqW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}